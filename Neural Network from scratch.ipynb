{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Inspired by [Machine Learning from Andrew Ng](https://www.coursera.org/learn/machine-learning) on Coursera and the wonderful [blogs](http://iamtrask.github.io/2015/07/27/python-network-part2/) from Trask. I am attempting to implement the simple neural networks from my understanding and memory. This is the best way to understand the architectures of the network and the mechanism behind back propagation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid function and its derivative function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "def sigmoid_out_derivative(out):\n",
    "    return out * (1 - out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy data example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n",
    "y = np.array([[0,1,1,0]]).T\n",
    "# y = np.array([[0,0,1,1]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 3), (4, 1))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Layer Neural Network \n",
    "This is almost as the same as Logistic Regression. And the training result is not promising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output after training:\n",
      "[[ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]\n",
      " [ 0.5]]\n",
      "Weights for the neural network\n",
      "[[  2.44488557e-16]\n",
      " [  2.47109102e-16]\n",
      " [ -3.30474445e-16]]\n"
     ]
    }
   ],
   "source": [
    "# 2 Layer Neural Network\n",
    "\n",
    "# initial weights with mean of 0\n",
    "Weights0 = np.random.randn(X.shape[1], 1) / np.sqrt(X.shape[1])\n",
    "# define the learning rate of Gradient descent\n",
    "alpha = 0.1\n",
    "\n",
    "for j in range(60000):\n",
    "    # forward propagation to compute the prediction\n",
    "    l0 = X\n",
    "    l1 = sigmoid(l0.dot(Weights0))\n",
    "    \n",
    "    # error between the target and the prediction\n",
    "    l1_error = l1 - y\n",
    "    \n",
    "    ## back propagation\n",
    "    \n",
    "    # multiply the error by the slope of the sigmoid at the values of l1\n",
    "    l1_delta = l1_error * sigmoid_out_derivative(l1)\n",
    "#     l1_delta = l1_error\n",
    "    \n",
    "    # update the weights\n",
    "    w0_derivative = l0.T.dot(l1_delta)\n",
    "    Weights0 -= alpha * w0_derivative   \n",
    "    \n",
    "print(\"Output after training:\")\n",
    "print(l1)\n",
    "print(\"Weights for the neural network\")\n",
    "print(Weights0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Layer Neural Network\n",
    "With just one more hidden layer, we can see the power of neural network especially for data with non-linear decision boudary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output after training:\n",
      "[[ 0.00637546]\n",
      " [ 0.99274119]\n",
      " [ 0.99250258]\n",
      " [ 0.00817472]]\n"
     ]
    }
   ],
   "source": [
    "# 3 Layer Neural Network\n",
    "\n",
    "# initial weights with mean of 0\n",
    "Weights0 = np.random.randn(X.shape[1], 10) / np.sqrt(X.shape[1])\n",
    "Weights1 = np.random.randn(10, 1) / np.sqrt(4)\n",
    "# define the learning rate of Gradient descent\n",
    "alpha = 0.25\n",
    "\n",
    "for j in range(60000):\n",
    "    # forward propagation to compute the prediction\n",
    "    l0 = X\n",
    "    l1 = sigmoid(l0.dot(Weights0))\n",
    "    l2 = sigmoid(l1.dot(Weights1))\n",
    "    \n",
    "    # error between the target and the prediction\n",
    "    l2_error = l2 - y\n",
    "    \n",
    "    ## back propagation\n",
    "    \n",
    "    # multiply the error by the slope of the sigmoid at the values of l2\n",
    "    l2_delta = l2_error * sigmoid_out_derivative(l2)\n",
    "    \n",
    "    # back propagate the error to layer 1\n",
    "    l1_error = l2_delta.dot(Weights1.T)\n",
    "    \n",
    "    # multiply the error by the slope of the sigmoid at the values of l1\n",
    "    l1_delta = l1_error * sigmoid_out_derivative(l1)\n",
    "    \n",
    "    # update the weights for the layers\n",
    "    w0_derivative = l0.T.dot(l1_delta)\n",
    "    Weights0 -= alpha * w0_derivative\n",
    "    \n",
    "    w1_derivative = l1.T.dot(l2_delta)\n",
    "    Weights1 -= alpha * w1_derivative\n",
    "    \n",
    "print(\"Output after training:\")\n",
    "print(l2)\n",
    "# print(\"Weights for the neural network\")\n",
    "# print(Weights0)\n",
    "# print(Weights1)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
